{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Model summary: 157 layers, 7047883 parameters, 0 gradients, 15.9 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Text: B  8 LONG\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import easyocr\n",
    "from pathlib import Path\n",
    "\n",
    "import pathlib\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath\n",
    "\n",
    "# Assuming YOLOv5's utils and models are in the Python path\n",
    "from models.experimental import attempt_load\n",
    "from utils.general import non_max_suppression, scale_boxes\n",
    "from utils.augmentations import letterbox\n",
    "\n",
    "# Function to load the YOLO model and detect objects\n",
    "def load_model(weights_path):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = attempt_load(weights_path, device=device)\n",
    "    stride = int(model.stride.max())\n",
    "    return model, stride, device\n",
    "\n",
    "def detect_and_crop_objects(model, stride, device, image_path, conf_thres=0.25, iou_thres=0.45, imgsz=640):\n",
    "    img0 = cv2.imread(image_path)\n",
    "    assert img0 is not None, f'Image Not Found {image_path}'\n",
    "\n",
    "    img = letterbox(img0, imgsz, stride=stride)[0]\n",
    "    img = img[:, :, ::-1].transpose(2, 0, 1)\n",
    "    img = np.ascontiguousarray(img)\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.float() / 255.0  # Normalize\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "    pred = model(img, augment=False)[0]\n",
    "    pred = non_max_suppression(pred, conf_thres, iou_thres)\n",
    "\n",
    "    cropped_images = []\n",
    "    for i, det in enumerate(pred):\n",
    "        if len(det):\n",
    "            det[:, :4] = scale_boxes(img.shape[2:], det[:, :4], img0.shape).round()\n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "                x1, y1, x2, y2 = map(int, xyxy)\n",
    "                cropped_img = img0[y1:y2, x1:x2]\n",
    "                cropped_images.append(cropped_img)\n",
    "    return cropped_images\n",
    "\n",
    "def apply_mask_for_ocr(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    normalized_gray = cv2.normalize(gray, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "    _, img_plate_bw = cv2.threshold(normalized_gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    img_plate_bw = cv2.morphologyEx(img_plate_bw, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "\n",
    "    # Find contours and filter them\n",
    "    contours, _ = cv2.findContours(img_plate_bw, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    filtered_contours = []\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area < 100: continue\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        aspect_ratio = float(w) / h\n",
    "        if 0.2 < aspect_ratio < 1.0 and gray.shape[0] * 0.3 < h < gray.shape[0] * 0.8 and gray.shape[1] * 0.02 < w < gray.shape[1] * 0.15:\n",
    "            filtered_contours.append(contour)\n",
    "\n",
    "    # Create a blank mask that matches the image size\n",
    "    mask = np.zeros_like(gray)  # Use gray instead of img\n",
    "\n",
    "    # Draw the filtered contours on the mask with white color and full opacity\n",
    "    cv2.drawContours(mask, filtered_contours, -1, (255, 255, 255), thickness=cv2.FILLED)\n",
    "\n",
    "    # Apply the mask to the original image\n",
    "    img_masked = cv2.bitwise_and(image, image, mask=mask)  # Use image instead of img\n",
    "    \n",
    "    return img_masked\n",
    "\n",
    "def extract_text_from_image_with_masking(cropped_images):\n",
    "    reader = easyocr.Reader(['en'])\n",
    "    combined_text = \"\"\n",
    "    for img in cropped_images:\n",
    "        masked_img = apply_mask_for_ocr(img)\n",
    "        results = reader.readtext(masked_img)\n",
    "        for (_, text, _) in results:\n",
    "            combined_text += text + \" \"\n",
    "    return combined_text.strip()\n",
    "\n",
    "# Then, in your process_image function or wherever appropriate, call this new function instead:\n",
    "def process_image_with_masking(image_path, weights_path):\n",
    "    model, stride, device = load_model(weights_path)\n",
    "    cropped_images = detect_and_crop_objects(model, stride, device, image_path)\n",
    "    detected_text = extract_text_from_image_with_masking(cropped_images)\n",
    "    return detected_text\n",
    "\n",
    "# Example usage\n",
    "weights_path = './runs/train/exp3/weights/best.pt'  # Update this path\n",
    "image_path = '../4224933456.jpg'  # Update this path\n",
    "detected_text = process_image_with_masking(image_path, weights_path)\n",
    "print(f\"Detected Text: {detected_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --image_path IMAGE_PATH --weights_path\n",
      "                             WEIGHTS_PATH\n",
      "ipykernel_launcher.py: error: the following arguments are required: --image_path, --weights_path\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faturrahman\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import easyocr\n",
    "from models.experimental import attempt_load\n",
    "from utils.general import non_max_suppression, scale_boxes\n",
    "from utils.augmentations import letterbox\n",
    "import argparse\n",
    "\n",
    "import pathlib\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath\n",
    "\n",
    "def load_model(weights_path):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = attempt_load(weights_path, device=device)\n",
    "    stride = int(model.stride.max())\n",
    "    return model, stride, device\n",
    "\n",
    "def detect_and_crop_objects(model, stride, device, image_path, conf_thres=0.25, iou_thres=0.45, imgsz=640):\n",
    "    img0 = cv2.imread(image_path)\n",
    "    assert img0 is not None, f'Image Not Found {image_path}'\n",
    "\n",
    "    img = letterbox(img0, imgsz, stride=stride)[0]\n",
    "    img = img[:, :, ::-1].transpose(2, 0, 1)\n",
    "    img = np.ascontiguousarray(img)\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.float() / 255.0  # Normalize\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "    pred = model(img, augment=False)[0]\n",
    "    pred = non_max_suppression(pred, conf_thres, iou_thres)\n",
    "\n",
    "    cropped_images = []\n",
    "    for i, det in enumerate(pred):\n",
    "        if len(det):\n",
    "            det[:, :4] = scale_boxes(img.shape[2:], det[:, :4], img0.shape).round()\n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "                x1, y1, x2, y2 = map(int, xyxy)\n",
    "                cropped_img = img0[y1:y2, x1:x2]\n",
    "                cropped_images.append(cropped_img)\n",
    "    return cropped_images\n",
    "\n",
    "def apply_mask_for_ocr(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    normalized_gray = cv2.normalize(gray, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "    _, img_plate_bw = cv2.threshold(normalized_gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    img_plate_bw = cv2.morphologyEx(img_plate_bw, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "\n",
    "    contours, _ = cv2.findContours(img_plate_bw, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    filtered_contours = []\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area < 100: continue\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        aspect_ratio = float(w) / h\n",
    "        if 0.2 < aspect_ratio < 1.0 and gray.shape[0] * 0.3 < h < gray.shape[0] * 0.8 and gray.shape[1] * 0.02 < w < gray.shape[1] * 0.15:\n",
    "            filtered_contours.append(contour)\n",
    "\n",
    "    mask = np.zeros_like(gray)\n",
    "    cv2.drawContours(mask, filtered_contours, -1, (255, 255, 255), thickness=cv2.FILLED)\n",
    "    img_masked = cv2.bitwise_and(image, image, mask=mask)\n",
    "    return img_masked\n",
    "\n",
    "def extract_text_from_image_with_masking(cropped_images):\n",
    "    reader = easyocr.Reader(['en'], gpu=True if torch.cuda.is_available() else False)\n",
    "    combined_text = \"\"\n",
    "    for img in cropped_images:\n",
    "        masked_img = apply_mask_for_ocr(img)\n",
    "        results = reader.readtext(masked_img)\n",
    "        for (_, text, _) in results:\n",
    "            combined_text += text + \" \"\n",
    "    return combined_text.strip()\n",
    "\n",
    "def process_image_with_masking(image_path, weights_path):\n",
    "    model, stride, device = load_model(weights_path)\n",
    "    cropped_images = detect_and_crop_objects(model, stride, device, image_path)\n",
    "    detected_text = extract_text_from_image_with_masking(cropped_images)\n",
    "    return detected_text\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Detect and extract text from images.\")\n",
    "    parser.add_argument(\"--image_path\", type=str, required=True, help=\"Path to the input image\")\n",
    "    parser.add_argument(\"--weights_path\", type=str, required=True, help=\"Path to the YOLO model weights\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    detected_text = process_image_with_masking(args.image_path, args.weights_path)\n",
    "    print(f\"Detected Text: {detected_text}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Faturrahman\\Downloads\\Bahan_Skripsi\\yolov5\\models\\yolo.py:242: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if augment:\n",
      "c:\\Users\\Faturrahman\\Downloads\\Bahan_Skripsi\\yolov5\\models\\yolo.py:153: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if profile:\n",
      "c:\\Users\\Faturrahman\\Downloads\\Bahan_Skripsi\\yolov5\\models\\yolo.py:157: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if visualize:\n",
      "c:\\Users\\Faturrahman\\Downloads\\Bahan_Skripsi\\yolov5\\models\\yolo.py:157: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if visualize:\n",
      "c:\\Users\\Faturrahman\\Downloads\\Bahan_Skripsi\\yolov5\\models\\yolo.py:153: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if profile:\n",
      "c:\\Users\\Faturrahman\\Downloads\\Bahan_Skripsi\\yolov5\\models\\yolo.py:98: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'onnx_tf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 30\u001b[0m\n\u001b[0;32m     19\u001b[0m torch\u001b[38;5;241m.\u001b[39monnx\u001b[38;5;241m.\u001b[39mexport(model,               \u001b[38;5;66;03m# model being run\u001b[39;00m\n\u001b[0;32m     20\u001b[0m                   x,                   \u001b[38;5;66;03m# model input (or a tuple for multiple inputs)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m,        \u001b[38;5;66;03m# where to save the model\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m                   dynamic_axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m : {\u001b[38;5;241m0\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m},    \u001b[38;5;66;03m# variable length axes\u001b[39;00m\n\u001b[0;32m     28\u001b[0m                                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m : {\u001b[38;5;241m0\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m}})\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01monnx\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01monnx_tf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prepare\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Load the ONNX file\u001b[39;00m\n\u001b[0;32m     33\u001b[0m model \u001b[38;5;241m=\u001b[39m onnx\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.onnx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'onnx_tf'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "\n",
    "import pathlib\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath\n",
    "\n",
    "# Path to your YOLOv5 model file\n",
    "model_path = './runs/train/exp3/weights/best.pt'\n",
    "\n",
    "# Load the model\n",
    "model = torch.load(model_path)['model'].float()  # Casting to float to ensure compatibility\n",
    "model.eval()\n",
    "\n",
    "# Dummy input to the model\n",
    "x = torch.randn(1, 3, 224, 224)  # Adjust the size according to your model's requirement\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(model,               # model being run\n",
    "                  x,                   # model input (or a tuple for multiple inputs)\n",
    "                  \"model.onnx\",        # where to save the model\n",
    "                  export_params=True,  # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,    # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                'output' : {0 : 'batch_size'}})\n",
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "\n",
    "# Load the ONNX file\n",
    "model = onnx.load('model.onnx')\n",
    "\n",
    "# Import the ONNX model to TensorFlow\n",
    "tf_rep = prepare(model)\n",
    "\n",
    "# Export the TensorFlow model\n",
    "tf_rep.export_graph('model_tf')\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the TensorFlow model\n",
    "saved_model_dir = 'model_tf'\n",
    "\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "DecodeError",
     "evalue": "Error parsing message with type 'tensorflow.GraphDef'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDecodeError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m model_proto \u001b[38;5;241m=\u001b[39m onnx_model\u001b[38;5;241m.\u001b[39mSerializeToString()\n\u001b[0;32m     13\u001b[0m graph_def \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mGraph()\u001b[38;5;241m.\u001b[39mas_graph_def()\n\u001b[1;32m---> 14\u001b[0m \u001b[43mgraph_def\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParseFromString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_proto\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Set input/output names and shapes if necessary\u001b[39;00m\n\u001b[0;32m     17\u001b[0m input_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput:0\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Adjust based on your model's input name\u001b[39;00m\n",
      "\u001b[1;31mDecodeError\u001b[0m: Error parsing message with type 'tensorflow.GraphDef'"
     ]
    }
   ],
   "source": [
    "import tf2onnx\n",
    "import onnx\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(\"model.onnx\")\n",
    "\n",
    "# Convert to TensorFlow\n",
    "spec = (tf.TensorSpec((None, 224, 224, 3), tf.float32, name=\"input\"),)  # Adjust the shape according to your input\n",
    "output_path = \"model.pb\"  # Or use .saved_model for SavedModel format\n",
    "model_proto, external_tensor_storage = tf2onnx.convert.from_onnx(\n",
    "    onnx_model,\n",
    "    input_signature=spec,\n",
    "    opset=13,  # Adjust based on the opset version used for ONNX export\n",
    "    output_path=output_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faturrahman\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\Faturrahman\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete. The TFLite model is saved at: model.tflite\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Convert ONNX model to TensorFlow\n",
    "def convert_onnx_to_tf(onnx_path, tf_path):\n",
    "    # Load the ONNX model\n",
    "    onnx_model = onnx.load(onnx_path)\n",
    "\n",
    "    # Convert to TensorFlow\n",
    "    tf_rep = prepare(onnx_model)\n",
    "\n",
    "    # Export the TensorFlow model\n",
    "    tf_rep.export_graph(tf_path)\n",
    "\n",
    "# Step 2: Convert TensorFlow model to TensorFlow Lite\n",
    "def convert_tf_to_tflite(tf_path, tflite_path):\n",
    "    # Convert the TensorFlow model to TensorFlow Lite\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    # Save the TFLite model\n",
    "    with open(tflite_path, \"wb\") as f_out:\n",
    "        f_out.write(tflite_model)\n",
    "\n",
    "# Paths for the models\n",
    "onnx_model_path = \"model.onnx\"  # Change this to the path of your ONNX model\n",
    "tf_model_path = \"./model.tf\"  # Change this to where you want to save the TensorFlow model\n",
    "tflite_model_path = \"model.tflite\"  # Change this to where you want to save the TFLite model\n",
    "\n",
    "# Conversion\n",
    "convert_onnx_to_tf(onnx_model_path, tf_model_path)\n",
    "convert_tf_to_tflite(tf_model_path, tflite_model_path)\n",
    "\n",
    "print(\"Conversion complete. The TFLite model is saved at:\", tflite_model_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
